{"nbformat": 4, "nbformat_minor": 5, "metadata": {}, "cells": [{"id": "df43f374", "cell_type": "markdown", "source": "# Data Preprocessing & Feature Engineering - Adult Dataset (Jupyter Notebook)\n\nThis notebook follows the assignment instructions from **EDA2.docx** and uses the provided **adult_with_headers.csv** dataset.\n\nIt covers:\n- Data exploration\n- Missing value handling\n- Scaling (Standard & Min-Max)\n- Encoding techniques\n- Feature engineering\n- Outlier detection (Isolation Forest)\n- Feature relationship analysis (PPS vs Correlation)\n\n---", "metadata": {}}, {"id": "535292ce", "cell_type": "markdown", "source": "## Step 1: Install & Import Required Libraries", "metadata": {}}, {"id": "6341333a", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "\n# If needed, install dependencies (run once)\n# !pip install ppscore scikit-learn seaborn\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\nfrom sklearn.ensemble import IsolationForest\nimport ppscore as pps\n", "outputs": []}, {"id": "6f10f303", "cell_type": "markdown", "source": "## Step 2: Load Dataset\n\nMake sure `adult_with_headers.csv` is in the same folder as this notebook.", "metadata": {}}, {"id": "e1645148", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "\n# Load dataset\ndf = pd.read_csv('adult_with_headers.csv')\n\ndf.head()\n", "outputs": []}, {"id": "733aea37", "cell_type": "markdown", "source": "## Step 3: Data Exploration", "metadata": {}}, {"id": "8535de8a", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "\n# Dataset info\ndf.info()\n\n# Summary statistics\ndf.describe()\n", "outputs": []}, {"id": "e27bad70", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "\n# Check missing values\ndf.isnull().sum()\n", "outputs": []}, {"id": "25c5b237", "cell_type": "markdown", "source": "## Step 4: Handle Missing Values", "metadata": {}}, {"id": "6f39699f", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "\n# Replace '?' with NaN if present\ndf.replace('?', np.nan, inplace=True)\n\n# Numerical columns - fill with median\nnum_cols = df.select_dtypes(include=['int64', 'float64']).columns\ndf[num_cols] = df[num_cols].fillna(df[num_cols].median())\n\n# Categorical columns - fill with mode\ncat_cols = df.select_dtypes(include=['object']).columns\nfor col in cat_cols:\n    df[col].fillna(df[col].mode()[0], inplace=True)\n\ndf.isnull().sum()\n", "outputs": []}, {"id": "1208b108", "cell_type": "markdown", "source": "## Step 5: Feature Scaling\n\n**Standard Scaling:** Used for models like Logistic Regression, SVM, PCA\n\n**Min-Max Scaling:** Used for distance-based models like KNN and Neural Networks", "metadata": {}}, {"id": "75906935", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "\nstandard_scaler = StandardScaler()\nminmax_scaler = MinMaxScaler()\n\ndf_standard_scaled = df.copy()\ndf_minmax_scaled = df.copy()\n\ndf_standard_scaled[num_cols] = standard_scaler.fit_transform(df[num_cols])\ndf_minmax_scaled[num_cols] = minmax_scaler.fit_transform(df[num_cols])\n\ndf_standard_scaled.head()\n", "outputs": []}, {"id": "dba9230a", "cell_type": "markdown", "source": "## Step 6: Encoding Techniques", "metadata": {}}, {"id": "f2464859", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "\ndf_encoded = df.copy()\nlabel_enc = LabelEncoder()\n\nfor col in cat_cols:\n    if df[col].nunique() < 5:\n        df_encoded = pd.get_dummies(df_encoded, columns=[col], drop_first=True)\n    else:\n        df_encoded[col] = label_enc.fit_transform(df[col])\n\ndf_encoded.head()\n", "outputs": []}, {"id": "ae810d01", "cell_type": "markdown", "source": "## Step 7: Feature Engineering", "metadata": {}}, {"id": "7a5f23f9", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "\n# Feature 1: Age Group\ndf_encoded['Age_Group'] = pd.cut(\n    df['age'],\n    bins=[0, 25, 40, 60, 100],\n    labels=['Young', 'Adult', 'Mid-Age', 'Senior']\n)\n\n# Feature 2: Capital Gain Indicator\ndf_encoded['High_Capital_Gain'] = (df['capital-gain'] > 0).astype(int)\n\ndf_encoded.head()\n", "outputs": []}, {"id": "a45f30ad", "cell_type": "markdown", "source": "## Step 7.1: Log Transformation for Skewed Feature", "metadata": {}}, {"id": "f2062508", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "\ndf_encoded['Log_Capital_Gain'] = np.log1p(df['capital-gain'])\n\nsns.histplot(df['capital-gain'], bins=50)\nplt.title(\"Original Capital Gain Distribution\")\nplt.show()\n\nsns.histplot(df_encoded['Log_Capital_Gain'], bins=50)\nplt.title(\"Log-Transformed Capital Gain Distribution\")\nplt.show()\n", "outputs": []}, {"id": "d05a8634", "cell_type": "markdown", "source": "## Step 8: Outlier Detection using Isolation Forest", "metadata": {}}, {"id": "7309fd05", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "\niso_forest = IsolationForest(contamination=0.05, random_state=42)\n\noutlier_labels = iso_forest.fit_predict(df_encoded.select_dtypes(include=['int64', 'float64']))\n\ndf_encoded['Outlier'] = outlier_labels\n\ndf_cleaned = df_encoded[df_encoded['Outlier'] == 1]\n\ndf_cleaned.shape\n", "outputs": []}, {"id": "e3f22dee", "cell_type": "markdown", "source": "## Step 9: PPS vs Correlation Matrix", "metadata": {}}, {"id": "161eb21a", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "\npps_matrix = pps.matrix(df_cleaned)\n\npps_matrix.head()\n", "outputs": []}, {"id": "51d81e62", "cell_type": "markdown", "source": "## Step 9.1: Correlation Matrix", "metadata": {}}, {"id": "a92c7ca3", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "\nplt.figure(figsize=(10, 6))\nsns.heatmap(df_cleaned.select_dtypes(include=['int64', 'float64']).corr(), cmap='coolwarm')\nplt.title(\"Correlation Matrix\")\nplt.show()\n", "outputs": []}, {"id": "faea2ce1", "cell_type": "markdown", "source": "## Final Conclusion\n\nThis Jupyter notebook fulfills all assignment objectives from EDA2.docx, including **preprocessing, feature engineering, scaling, encoding, outlier detection, and feature relationship analysis**.", "metadata": {}}]}